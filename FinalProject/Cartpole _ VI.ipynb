{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CARTPOLE problem solved using Value-Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[INFO]: Min State: 7 Max State: 1000 Num States: 2401\n",
      "Reward: 16.0, Average reward over 0 trials: 16.0\n",
      "Reward: 18.0, Average reward over 1 trials: 17.0\n",
      "Reward: 32.0, Average reward over 2 trials: 22.0\n",
      "Reward: 25.0, Average reward over 3 trials: 22.75\n",
      "Reward: 45.0, Average reward over 4 trials: 27.2\n",
      "Reward: 11.0, Average reward over 5 trials: 24.5\n",
      "Reward: 8.0, Average reward over 6 trials: 22.1428571429\n",
      "Reward: 200.0, Average reward over 7 trials: 44.375\n",
      "Reward: 43.0, Average reward over 8 trials: 44.2222222222\n",
      "Reward: 106.0, Average reward over 9 trials: 50.4\n",
      "Reward: 31.0, Average reward over 10 trials: 48.6363636364\n",
      "Reward: 131.0, Average reward over 11 trials: 55.5\n",
      "Reward: 185.0, Average reward over 12 trials: 65.4615384615\n",
      "Reward: 64.0, Average reward over 13 trials: 65.3571428571\n",
      "Reward: 34.0, Average reward over 14 trials: 63.2666666667\n",
      "Reward: 128.0, Average reward over 15 trials: 67.3125\n",
      "Reward: 34.0, Average reward over 16 trials: 65.3529411765\n",
      "Reward: 34.0, Average reward over 17 trials: 63.6111111111\n",
      "Reward: 44.0, Average reward over 18 trials: 62.5789473684\n",
      "Reward: 52.0, Average reward over 19 trials: 62.05\n",
      "Reward: 43.0, Average reward over 20 trials: 61.1428571429\n",
      "Reward: 30.0, Average reward over 21 trials: 59.7272727273\n",
      "Reward: 52.0, Average reward over 22 trials: 59.3913043478\n",
      "Reward: 34.0, Average reward over 23 trials: 58.3333333333\n",
      "Reward: 58.0, Average reward over 24 trials: 58.32\n",
      "Reward: 48.0, Average reward over 25 trials: 57.9230769231\n",
      "Reward: 29.0, Average reward over 26 trials: 56.8518518519\n",
      "Reward: 28.0, Average reward over 27 trials: 55.8214285714\n",
      "Reward: 30.0, Average reward over 28 trials: 54.9310344828\n",
      "Reward: 27.0, Average reward over 29 trials: 54.0\n",
      "Reward: 37.0, Average reward over 30 trials: 53.4516129032\n",
      "Reward: 21.0, Average reward over 31 trials: 52.4375\n",
      "Reward: 21.0, Average reward over 32 trials: 51.4848484848\n",
      "Reward: 25.0, Average reward over 33 trials: 50.7058823529\n",
      "Reward: 31.0, Average reward over 34 trials: 50.1428571429\n",
      "Reward: 31.0, Average reward over 35 trials: 49.6111111111\n",
      "Reward: 120.0, Average reward over 36 trials: 51.5135135135\n",
      "Reward: 11.0, Average reward over 37 trials: 50.4473684211\n",
      "Reward: 57.0, Average reward over 38 trials: 50.6153846154\n",
      "Reward: 128.0, Average reward over 39 trials: 52.55\n",
      "Reward: 75.0, Average reward over 40 trials: 53.0975609756\n",
      "Reward: 128.0, Average reward over 41 trials: 54.880952381\n",
      "Reward: 157.0, Average reward over 42 trials: 57.2558139535\n",
      "Reward: 167.0, Average reward over 43 trials: 59.75\n",
      "Reward: 65.0, Average reward over 44 trials: 59.8666666667\n",
      "Reward: 108.0, Average reward over 45 trials: 60.9130434783\n",
      "Reward: 118.0, Average reward over 46 trials: 62.1276595745\n",
      "Reward: 120.0, Average reward over 47 trials: 63.3333333333\n",
      "Reward: 200.0, Average reward over 48 trials: 66.1224489796\n",
      "Reward: 176.0, Average reward over 49 trials: 68.32\n",
      "Reward: 178.0, Average reward over 50 trials: 70.4705882353\n",
      "Reward: 162.0, Average reward over 51 trials: 72.2307692308\n",
      "Reward: 190.0, Average reward over 52 trials: 74.4528301887\n",
      "Reward: 47.0, Average reward over 53 trials: 73.9444444444\n",
      "Reward: 108.0, Average reward over 54 trials: 74.5636363636\n",
      "Reward: 197.0, Average reward over 55 trials: 76.75\n",
      "Reward: 200.0, Average reward over 56 trials: 78.9122807018\n",
      "Reward: 127.0, Average reward over 57 trials: 79.7413793103\n",
      "Reward: 111.0, Average reward over 58 trials: 80.2711864407\n",
      "Reward: 192.0, Average reward over 59 trials: 82.1333333333\n",
      "Reward: 169.0, Average reward over 60 trials: 83.5573770492\n",
      "Reward: 198.0, Average reward over 61 trials: 85.4032258065\n",
      "Reward: 33.0, Average reward over 62 trials: 84.5714285714\n",
      "Reward: 62.0, Average reward over 63 trials: 84.21875\n",
      "Reward: 195.0, Average reward over 64 trials: 85.9230769231\n",
      "Reward: 30.0, Average reward over 65 trials: 85.0757575758\n",
      "Reward: 165.0, Average reward over 66 trials: 86.2686567164\n",
      "Reward: 15.0, Average reward over 67 trials: 85.2205882353\n",
      "Reward: 159.0, Average reward over 68 trials: 86.2898550725\n",
      "Reward: 200.0, Average reward over 69 trials: 87.9142857143\n",
      "Reward: 48.0, Average reward over 70 trials: 87.3521126761\n",
      "Reward: 200.0, Average reward over 71 trials: 88.9166666667\n",
      "Reward: 40.0, Average reward over 72 trials: 88.2465753425\n",
      "Reward: 32.0, Average reward over 73 trials: 87.4864864865\n",
      "Reward: 94.0, Average reward over 74 trials: 87.5733333333\n",
      "Reward: 109.0, Average reward over 75 trials: 87.8552631579\n",
      "Reward: 161.0, Average reward over 76 trials: 88.8051948052\n",
      "Reward: 141.0, Average reward over 77 trials: 89.4743589744\n",
      "Reward: 84.0, Average reward over 78 trials: 89.4050632911\n",
      "Reward: 96.0, Average reward over 79 trials: 89.4875\n",
      "Reward: 93.0, Average reward over 80 trials: 89.5308641975\n",
      "Reward: 105.0, Average reward over 81 trials: 89.7195121951\n",
      "Reward: 113.0, Average reward over 82 trials: 90.0\n",
      "Reward: 173.0, Average reward over 83 trials: 90.9880952381\n",
      "Reward: 150.0, Average reward over 84 trials: 91.6823529412\n",
      "Reward: 106.0, Average reward over 85 trials: 91.8488372093\n",
      "Reward: 115.0, Average reward over 86 trials: 92.1149425287\n",
      "Reward: 147.0, Average reward over 87 trials: 92.7386363636\n",
      "Reward: 152.0, Average reward over 88 trials: 93.404494382\n",
      "Reward: 183.0, Average reward over 89 trials: 94.4\n",
      "Reward: 124.0, Average reward over 90 trials: 94.7252747253\n",
      "Reward: 200.0, Average reward over 91 trials: 95.8695652174\n",
      "Reward: 99.0, Average reward over 92 trials: 95.9032258065\n",
      "Reward: 104.0, Average reward over 93 trials: 95.9893617021\n",
      "Reward: 103.0, Average reward over 94 trials: 96.0631578947\n",
      "Reward: 95.0, Average reward over 95 trials: 96.0520833333\n",
      "Reward: 103.0, Average reward over 96 trials: 96.1237113402\n",
      "Reward: 110.0, Average reward over 97 trials: 96.2653061224\n",
      "Reward: 178.0, Average reward over 98 trials: 97.0909090909\n",
      "Reward: 153.0, Average reward over 99 trials: 97.65\n",
      "Reward: 115.0, Average reward over 100 trials: 98.64\n",
      "Reward: 115.0, Average reward over 101 trials: 99.61\n",
      "Reward: 148.0, Average reward over 102 trials: 100.77\n",
      "Reward: 108.0, Average reward over 103 trials: 101.6\n",
      "Reward: 149.0, Average reward over 104 trials: 102.64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d0572038422c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mstate_transition_probabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_state_transition_probabilities_from_counters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_transition_probabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_transition_counters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mstate_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_value_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_transition_probabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-d0572038422c>\u001b[0m in \u001b[0;36mrun_value_iteration\u001b[0;34m(state_values, state_transition_probabilities, state_rewards)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mbest_action_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mbest_action_values\u001b[0m \u001b[0;34m=\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_action_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_transition_probabilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mstate_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_rewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbest_action_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Finite-state MDP solved using Value Iteration\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "observation = env.reset()\n",
    "\n",
    "\n",
    "num_observation_dimensions = np.size(observation)\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "observation_space_high = env.observation_space.high\n",
    "observation_space_low = env.observation_space.low\n",
    "\n",
    "\n",
    "# Hyperparameter\n",
    "num_bins_per_observation_dimension = 7 # Could try different number of bins for the different dimensions\n",
    "num_states = pow(num_bins_per_observation_dimension,num_observation_dimensions)\n",
    "\n",
    "def make_observation_bins(minV, maxV, num_bins):\n",
    "    if(minV == -np.Inf) or (minV <-10e8):\n",
    "        minV = -5 # Should really learn this const instead\n",
    "    if(maxV == np.Inf) or (minV>10e8):\n",
    "        maxV = 5\n",
    "    bins = np.arange(minV, maxV, (float(maxV)-float(minV))/((num_bins)-2))\n",
    "    bins = np.sort(np.append(bins, [0])) # Ensure we split at 0\n",
    "    \n",
    "    return bins\n",
    "\n",
    "observation_dimension_bins = []\n",
    "for observation_dimension in range(num_observation_dimensions):\n",
    "    observation_dimension_bins.append(make_observation_bins(observation_space_low[observation_dimension], \\\n",
    "                                                            observation_space_high[observation_dimension], \\\n",
    "                                                           num_bins_per_observation_dimension))\n",
    "    \n",
    "# print(observation_dimension_bins)\n",
    "\n",
    "def observation_to_state(observation):\n",
    "    state = 0\n",
    "    for observation_dimension in range(num_observation_dimensions):\n",
    "        state = state + np.digitize(observation[observation_dimension],observation_dimension_bins[observation_dimension]) \\\n",
    "        * pow(num_bins_per_observation_dimension, observation_dimension)\n",
    "        \n",
    "    return state\n",
    "  \n",
    "print(\"[INFO]: Min State: {} Max State: {} Num States: {}\".format(observation_to_state([-5,-5,-5,-5.5]), observation_to_state([5,5,5,5.5]),\n",
    "                                                          num_states))\n",
    "\n",
    "state_values = np.random.rand(num_states) * 0.1\n",
    "state_rewards = np.zeros((num_states))\n",
    "state_transition_probabilities = np.ones((num_states, num_states, num_actions)) / num_states\n",
    "state_transition_counters = np.zeros((num_states, num_states, num_actions))\n",
    "\n",
    "def pick_best_action(current_state, state_values, state_transition_probabilities):\n",
    "    best_action = -1\n",
    "    best_action_value = -np.Inf\n",
    "    for a_i in range(num_actions):\n",
    "        action_value = state_transition_probabilities[current_state,:,a_i].dot(state_values)\n",
    "        if (action_value > best_action_value):\n",
    "            best_action_value = action_value\n",
    "            best_action = a_i\n",
    "        elif (action_value == best_action_value):\n",
    "            if np.random.randint(0,2) == 0:\n",
    "                best_action = a_i\n",
    "            \n",
    "    return best_action\n",
    "\n",
    "\n",
    "def update_state_transition_probabilities_from_counters(probabilities, counters):\n",
    "    for a_i in range(num_actions):\n",
    "        for s_i in range(num_states):\n",
    "            total_transitions_out_of_state = np.sum(counters[s_i,:,a_i])\n",
    "            if(total_transitions_out_of_state > 0):\n",
    "                probabilities[s_i,:,a_i] = counters[s_i,:,a_i] / total_transitions_out_of_state\n",
    "            \n",
    "    return probabilities\n",
    "\n",
    "\n",
    "def run_value_iteration(state_values, state_transition_probabilities, state_rewards):\n",
    "    gamma = 0.995\n",
    "    convergence_tolerance = 0.01\n",
    "    iteration = 0\n",
    "    max_dif = np.Inf\n",
    "    while max_dif > convergence_tolerance:  \n",
    "        iteration = iteration + 1\n",
    "        old_state_values = np.copy(state_values)\n",
    "\n",
    "        best_action_values = np.zeros((num_states)) - np.Inf\n",
    "        for a_i in range(num_actions):\n",
    "            best_action_values = \\\n",
    "                np.maximum(best_action_values, state_transition_probabilities[:,:,a_i].dot(state_values))\n",
    "\n",
    "        state_values = state_rewards + gamma * best_action_values\n",
    "        max_dif = np.max(np.abs(state_values - old_state_values))       \n",
    "    \n",
    "    return state_values\n",
    "    \n",
    "          \n",
    "        \n",
    "episode_rewards = []\n",
    "# env.monitor.start('training_dir3', force=True)\n",
    "for i_episode in range(1000):\n",
    "    current_observation = env.reset()\n",
    "    current_state = observation_to_state(current_observation)\n",
    "    \n",
    "    episode_reward = 0\n",
    "    \n",
    "    for t in range(1000):\n",
    "        action = pick_best_action(current_state, state_values, state_transition_probabilities)\n",
    "        \n",
    "        old_state = current_state\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        current_state = observation_to_state(observation)\n",
    "        \n",
    "        state_transition_counters[old_state, current_state, action] = \\\n",
    "            state_transition_counters[old_state, current_state, action] + 1\n",
    "        \n",
    "        episode_reward = episode_reward + reward        \n",
    "        \n",
    "        if done:\n",
    "            episode_rewards.append(episode_reward)\n",
    "            print(\"Reward: {}, Average reward over {} trials: {}\".format(episode_reward, i_episode, np.mean(episode_rewards[-100:])))            \n",
    "            \n",
    "            if(t < 195):\n",
    "                reward = -1\n",
    "            else:\n",
    "                reward = 0\n",
    "            state_rewards[current_state] = reward\n",
    "\n",
    "            state_transition_probabilities = update_state_transition_probabilities_from_counters(state_transition_probabilities, state_transition_counters)\n",
    "            state_values = run_value_iteration(state_values, state_transition_probabilities, state_rewards)\n",
    "            break\n",
    "            \n",
    "#env.monitor.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
