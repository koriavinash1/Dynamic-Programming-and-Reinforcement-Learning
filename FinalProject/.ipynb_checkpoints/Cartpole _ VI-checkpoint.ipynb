{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CARTPOLE problem solved using Value-Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[INFO]: Min State: 7 Max State: 1000 Num States: 2401\n",
      "Reward: 16.0, Average reward over 0 trials: 16.0\n",
      "Reward: 18.0, Average reward over 1 trials: 17.0\n",
      "Reward: 32.0, Average reward over 2 trials: 22.0\n",
      "Reward: 25.0, Average reward over 3 trials: 22.75\n"
     ]
    }
   ],
   "source": [
    "# Finite-state MDP solved using Value Iteration\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "observation = env.reset()\n",
    "\n",
    "\n",
    "num_observation_dimensions = np.size(observation)\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "observation_space_high = env.observation_space.high\n",
    "observation_space_low = env.observation_space.low\n",
    "\n",
    "\n",
    "# Hyperparameter\n",
    "num_bins_per_observation_dimension = 7 # Could try different number of bins for the different dimensions\n",
    "num_states = pow(num_bins_per_observation_dimension,num_observation_dimensions)\n",
    "\n",
    "def make_observation_bins(minV, maxV, num_bins):\n",
    "    if(minV == -np.Inf) or (minV <-10e8):\n",
    "        minV = -5 # Should really learn this const instead\n",
    "    if(maxV == np.Inf) or (minV>10e8):\n",
    "        maxV = 5\n",
    "    bins = np.arange(minV, maxV, (float(maxV)-float(minV))/((num_bins)-2))\n",
    "    bins = np.sort(np.append(bins, [0])) # Ensure we split at 0\n",
    "    \n",
    "    return bins\n",
    "\n",
    "observation_dimension_bins = []\n",
    "for observation_dimension in range(num_observation_dimensions):\n",
    "    observation_dimension_bins.append(make_observation_bins(observation_space_low[observation_dimension], \\\n",
    "                                                            observation_space_high[observation_dimension], \\\n",
    "                                                           num_bins_per_observation_dimension))\n",
    "    \n",
    "# print(observation_dimension_bins)\n",
    "\n",
    "def observation_to_state(observation):\n",
    "    state = 0\n",
    "    for observation_dimension in range(num_observation_dimensions):\n",
    "        state = state + np.digitize(observation[observation_dimension],observation_dimension_bins[observation_dimension]) \\\n",
    "        * pow(num_bins_per_observation_dimension, observation_dimension)\n",
    "        \n",
    "    return state\n",
    "  \n",
    "print(\"[INFO]: Min State: {} Max State: {} Num States: {}\".format(observation_to_state([-5,-5,-5,-5.5]), observation_to_state([5,5,5,5.5]),\n",
    "                                                          num_states))\n",
    "\n",
    "state_values = np.random.rand(num_states) * 0.1\n",
    "state_rewards = np.zeros((num_states))\n",
    "state_transition_probabilities = np.ones((num_states, num_states, num_actions)) / num_states\n",
    "state_transition_counters = np.zeros((num_states, num_states, num_actions))\n",
    "\n",
    "def pick_best_action(current_state, state_values, state_transition_probabilities):\n",
    "    best_action = -1\n",
    "    best_action_value = -np.Inf\n",
    "    for a_i in range(num_actions):\n",
    "        action_value = state_transition_probabilities[current_state,:,a_i].dot(state_values)\n",
    "        if (action_value > best_action_value):\n",
    "            best_action_value = action_value\n",
    "            best_action = a_i\n",
    "        elif (action_value == best_action_value):\n",
    "            if np.random.randint(0,2) == 0:\n",
    "                best_action = a_i\n",
    "            \n",
    "    return best_action\n",
    "\n",
    "\n",
    "def update_state_transition_probabilities_from_counters(probabilities, counters):\n",
    "    for a_i in range(num_actions):\n",
    "        for s_i in range(num_states):\n",
    "            total_transitions_out_of_state = np.sum(counters[s_i,:,a_i])\n",
    "            if(total_transitions_out_of_state > 0):\n",
    "                probabilities[s_i,:,a_i] = counters[s_i,:,a_i] / total_transitions_out_of_state\n",
    "            \n",
    "    return probabilities\n",
    "\n",
    "\n",
    "def run_value_iteration(state_values, state_transition_probabilities, state_rewards):\n",
    "    gamma = 0.995\n",
    "    convergence_tolerance = 0.01\n",
    "    iteration = 0\n",
    "    max_dif = np.Inf\n",
    "    while max_dif > convergence_tolerance:  \n",
    "        iteration = iteration + 1\n",
    "        old_state_values = np.copy(state_values)\n",
    "\n",
    "        best_action_values = np.zeros((num_states)) - np.Inf\n",
    "        for a_i in range(num_actions):\n",
    "            best_action_values = \\\n",
    "                np.maximum(best_action_values, state_transition_probabilities[:,:,a_i].dot(state_values))\n",
    "\n",
    "        state_values = state_rewards + gamma * best_action_values\n",
    "        max_dif = np.max(np.abs(state_values - old_state_values))       \n",
    "    \n",
    "    return state_values\n",
    "    \n",
    "          \n",
    "        \n",
    "episode_rewards = []\n",
    "# env.monitor.start('training_dir3', force=True)\n",
    "for i_episode in range(1000):\n",
    "    current_observation = env.reset()\n",
    "    current_state = observation_to_state(current_observation)\n",
    "    \n",
    "    episode_reward = 0\n",
    "    \n",
    "    for t in range(1000):\n",
    "        action = pick_best_action(current_state, state_values, state_transition_probabilities)\n",
    "        \n",
    "        old_state = current_state\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        current_state = observation_to_state(observation)\n",
    "        \n",
    "        state_transition_counters[old_state, current_state, action] = \\\n",
    "            state_transition_counters[old_state, current_state, action] + 1\n",
    "        \n",
    "        episode_reward = episode_reward + reward        \n",
    "        \n",
    "        if done:\n",
    "            episode_rewards.append(episode_reward)\n",
    "            print(\"Reward: {}, Average reward over {} trials: {}\".format(episode_reward, i_episode, np.mean(episode_rewards[-100:])))            \n",
    "            \n",
    "            if(t < 195):\n",
    "                reward = -1\n",
    "            else:\n",
    "                reward = 0\n",
    "            state_rewards[current_state] = reward\n",
    "\n",
    "            state_transition_probabilities = update_state_transition_probabilities_from_counters(state_transition_probabilities, state_transition_counters)\n",
    "            state_values = run_value_iteration(state_values, state_transition_probabilities, state_rewards)\n",
    "            break\n",
    "            \n",
    "#env.monitor.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
